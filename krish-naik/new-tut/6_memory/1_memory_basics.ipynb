{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# Define the LLM model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartikeya\\AppData\\Local\\Temp\\ipykernel_3432\\995385594.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartikeya\\AppData\\Local\\Temp\\ipykernel_3432\\2449831023.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there! I am Kartikeya\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Kartikeya! I'm Ada, your friendly AI companion. I've been designed to assist and converse with humans like you. I've been trained on a vast amount of text data, including books, articles, research papers, and even chat logs. My knowledge base is constantly updated to ensure I stay up-to-date on the latest developments in various fields. I'm excited to chat with you and learn more about your interests and thoughts! What's on your mind, Kartikeya?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hii there! I am Kartikeya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there! I am Kartikeya\n",
      "AI: Nice to meet you, Kartikeya! I'm Ada, your friendly AI companion. I've been designed to assist and converse with humans like you. I've been trained on a vast amount of text data, including books, articles, research papers, and even chat logs. My knowledge base is constantly updated to ensure I stay up-to-date on the latest developments in various fields. I'm excited to chat with you and learn more about your interests and thoughts! What's on your mind, Kartikeya?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's an easy one, Kartikeya! Your name is Kartikeya, which I've been told is a common Indian name with Sanskrit origins. In Hindu mythology, Kartikeya is also known as Murugan, the son of Shiva and the god of war. But don't worry, I'm not here to test your mythological knowledge! As your AI companion, I'm here to help you with any questions or topics you'd like to discuss. What would you like to talk about?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hii there! I am Kartikeya\n",
      "AI: Nice to meet you, Kartikeya! I'm Ada, your friendly AI companion. I've been designed to assist and converse with humans like you. I've been trained on a vast amount of text data, including books, articles, research papers, and even chat logs. My knowledge base is constantly updated to ensure I stay up-to-date on the latest developments in various fields. I'm excited to chat with you and learn more about your interests and thoughts! What's on your mind, Kartikeya?\n",
      "Human: What is my name?\n",
      "AI: That's an easy one, Kartikeya! Your name is Kartikeya, which I've been told is a common Indian name with Sanskrit origins. In Hindu mythology, Kartikeya is also known as Murugan, the son of Shiva and the god of war. But don't worry, I'm not here to test your mythological knowledge! As your AI companion, I'm here to help you with any questions or topics you'd like to discuss. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there! I am Kartiekeya\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Kartiekeya! I'm Ada, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I'm happy to chat with you about anything that's on your mind. By the way, I've been monitoring our conversation and I can tell you that it's currently 10:47 AM on a Wednesday in your time zone. How about we start with something relaxing, like the weather? Would you like to know the current temperature and humidity levels in your area?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = summary_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hii there! I am Kartiekeya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary: None (this is the beginning of the conversation)\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Hii there! I am Kartiekeya\n",
      "AI: Nice to meet you, Kartiekeya! I'm Ada, your friendly AI companion. I've been trained on a vast amount of text data, including books, articles, and conversations. I'm happy to chat with you about anything that's on your mind. By the way, I've been monitoring our conversation and I can tell you that it's currently 10:47 AM on a Wednesday in your time zone. How about we start with something relaxing, like the weather? Would you like to know the current temperature and humidity levels in your area?\n",
      "\n",
      "New summary: Kartiekeya introduces himself to Ada, a friendly AI companion, and Ada responds by introducing herself and offering to chat about various topics, including the weather.\n",
      "Human: Can you help me with writting a funny tweet?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A tweet! I'd be happy to help you with that, Kartiekeya! I've been trained on a vast amount of text data, including social media posts, so I think I can help you come up with a funny tweet.\\n\\nBefore I start generating some options, can you give me a bit more context? What kind of humor are you going for? Do you have a specific topic in mind or a particular tone you're aiming for? And how many characters are you looking to stay within?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you help me with writting a funny tweet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartikeya\\AppData\\Local\\Temp\\ipykernel_3432\\2788100847.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationBufferWindowMemory(k=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii there! I am Kartiekeya\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Kartiekeya! I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I can process and understand human language, and I'm happy to chat with you about anything that's on your mind. By the way, I've been trained on a massive dataset of text from the internet, which includes over 45 billion parameters and 1.4 billion tokens of text data. It's a lot of information, but I'm happy to help answer any questions you may have! What would you like to talk about, Kartiekeya?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window_memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "conversation_with_window = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_window.predict(input=\"Hii there! I am Kartiekeya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there! I am Kartiekeya\n",
      "AI: Nice to meet you, Kartiekeya! I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I can process and understand human language, and I'm happy to chat with you about anything that's on your mind. By the way, I've been trained on a massive dataset of text from the internet, which includes over 45 billion parameters and 1.4 billion tokens of text data. It's a lot of information, but I'm happy to help answer any questions you may have! What would you like to talk about, Kartiekeya?\n",
      "Human: Can you help me with writting a funny tweet?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A funny tweet, you say? I'd be delighted to help you with that, Kartiekeya! I've been trained on a vast amount of text data, including social media posts, articles, and even memes. I can generate humorous content based on current events, pop culture, or even your personal experiences.\\n\\nTo get started, can you give me a sense of what kind of humor you're looking for? Do you have a specific topic in mind, like a recent news story or a popular TV show? Or would you like me to suggest some prompts and we can brainstorm together?\\n\\nAlso, I'd like to mention that I've been trained on a dataset that includes many humorous tweets from various sources, including comedians, writers, and social media influencers. I can draw inspiration from those to help craft a funny tweet for you.\\n\\nLet me know how I can assist you, and we'll get started on creating a tweet that'll make people LOL!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_window.predict(input=\"Can you help me with writting a funny tweet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii there! I am Kartiekeya\n",
      "AI: Nice to meet you, Kartiekeya! I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I can process and understand human language, and I'm happy to chat with you about anything that's on your mind. By the way, I've been trained on a massive dataset of text from the internet, which includes over 45 billion parameters and 1.4 billion tokens of text data. It's a lot of information, but I'm happy to help answer any questions you may have! What would you like to talk about, Kartiekeya?\n",
      "Human: Can you help me with writting a funny tweet?\n",
      "AI: A funny tweet, you say? I'd be delighted to help you with that, Kartiekeya! I've been trained on a vast amount of text data, including social media posts, articles, and even memes. I can generate humorous content based on current events, pop culture, or even your personal experiences.\n",
      "\n",
      "To get started, can you give me a sense of what kind of humor you're looking for? Do you have a specific topic in mind, like a recent news story or a popular TV show? Or would you like me to suggest some prompts and we can brainstorm together?\n",
      "\n",
      "Also, I'd like to mention that I've been trained on a dataset that includes many humorous tweets from various sources, including comedians, writers, and social media influencers. I can draw inspiration from those to help craft a funny tweet for you.\n",
      "\n",
      "Let me know how I can assist you, and we'll get started on creating a tweet that'll make people LOL!\n",
      "Human: Memory in langchain keep it short\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Memory in Langchain, eh? So, you\\'re looking for a tweet that\\'s short, sweet, and to the point, just like the concept of memory in a Langchain! I love it!\\n\\nAs an AI, I\\'ve been trained on a vast amount of text data, including information about different types of memory, such as short-term, long-term, and episodic memory. But, I have to admit, I\\'m not familiar with the specific concept of \"memory in Langchain.\" Could you please tell me more about it? What does it refer to, and what kind of humor are you looking to convey?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_window.predict(input=\"Memory in langchain keep it short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Can you help me with writting a funny tweet?\n",
      "AI: A funny tweet, you say? I'd be delighted to help you with that, Kartiekeya! I've been trained on a vast amount of text data, including social media posts, articles, and even memes. I can generate humorous content based on current events, pop culture, or even your personal experiences.\n",
      "\n",
      "To get started, can you give me a sense of what kind of humor you're looking for? Do you have a specific topic in mind, like a recent news story or a popular TV show? Or would you like me to suggest some prompts and we can brainstorm together?\n",
      "\n",
      "Also, I'd like to mention that I've been trained on a dataset that includes many humorous tweets from various sources, including comedians, writers, and social media influencers. I can draw inspiration from those to help craft a funny tweet for you.\n",
      "\n",
      "Let me know how I can assist you, and we'll get started on creating a tweet that'll make people LOL!\n",
      "Human: Memory in langchain keep it short\n",
      "AI: Memory in Langchain, eh? So, you're looking for a tweet that's short, sweet, and to the point, just like the concept of memory in a Langchain! I love it!\n",
      "\n",
      "As an AI, I've been trained on a vast amount of text data, including information about different types of memory, such as short-term, long-term, and episodic memory. But, I have to admit, I'm not familiar with the specific concept of \"memory in Langchain.\" Could you please tell me more about it? What does it refer to, and what kind of humor are you looking to convey?\n",
      "Human: Agents in langchain keep it short\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agents in Langchain, I see! So, you\\'re looking for a tweet that\\'s short, snappy, and perhaps a bit sarcastic, given the use of the word \"agents\" which implies a sense of authority or control. I\\'m intrigued!\\n\\nAs a conversational AI, I\\'ve been trained on a wide range of texts, including scientific papers, news articles, and even chat logs. While I don\\'t have specific knowledge about Langchain, I can try to generate a tweet that\\'s humorous and relatable.\\n\\nHere\\'s a possible tweet:\\n\\n\"Agents in Langchain keeping it short... because they\\'re trying to avoid getting stuck in an infinite loop of meetings #LangchainLife #AgentProblems\"\\n\\nI drew inspiration from the concept of agents and the idea of Langchain being a complex system. The tweet pokes fun at the idea that agents might be trying to keep their interactions brief and to the point, possibly due to the complexity of the system they\\'re working within. The hashtags add a touch of humor and lightheartedness to the tweet.\\n\\nWhat do you think? Would you like me to make any changes or try a different approach?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_window.predict(input=\"Agents in langchain keep it short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartikeya\\AppData\\Local\\Temp\\ipykernel_3432\\530542540.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  window_memory = ConversationSummaryBufferMemory(llm=llm,max_token_limit=200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hii There! I'm Kartikeya\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\gen-ai\\langcain-tut\\new-tut\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\gen-ai\\langcain-tut\\new-tut\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kartikeya\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you Kartikeya! I'm Ada, your friendly AI assistant. I've been trained on a vast amount of text data, including books, articles, and conversations, which enables me to understand and respond to a wide range of topics. Did you know that I've been designed to learn and improve over time, so the more we chat, the more accurate and helpful I'll become? What brings you here today, Kartikeya?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "window_memory = ConversationSummaryBufferMemory(llm=llm,max_token_limit=200)\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=window_memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation_with_summary.predict(input=\"Hii There! I'm Kartikeya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hii There! I'm Kartikeya\n",
      "AI: Nice to meet you Kartikeya! I'm Ada, your friendly AI assistant. I've been trained on a vast amount of text data, including books, articles, and conversations, which enables me to understand and respond to a wide range of topics. Did you know that I've been designed to learn and improve over time, so the more we chat, the more accurate and helpful I'll become? What brings you here today, Kartikeya?\n",
      "Human: Agents in langchain keep it short\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agents in LangChain, you say? That\\'s a fascinating topic! LangChain is a popular AI model that\\'s known for its ability to generate human-like text based on the input it receives. Did you know that LangChain is a type of transformer-based model that uses a technique called masked language modeling to predict the missing words in a sentence?\\n\\nAs for agents in LangChain, I\\'d be happy to provide more information. LangChain uses a concept called \"agents\" to facilitate communication between different entities, such as humans and AI models. These agents act as intermediaries, allowing for seamless interaction between the different entities. They\\'re essentially like digital diplomats, if you will!\\n\\nHowever, I must admit that I don\\'t have any specific information on the \"keep it short\" aspect of LangChain agents. Could you please provide more context or clarify what you mean by that? I\\'m here to learn and help, so please feel free to share more!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Agents in langchain keep it short\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START , MessagesState , StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_app():\n",
    "    # Define new graph\n",
    "    workflow = StateGraph(state_schema=MessagesState)\n",
    "    \n",
    "    # Define a function that calls the model\n",
    "    def call_model(state : MessagesState):\n",
    "        response = llm.invoke(state['messages'])\n",
    "        return {\"messages\" : response}\n",
    "    \n",
    "    # Define the nodes we will cycle between\n",
    "    workflow.add_edge(START,\"model\")\n",
    "    workflow.add_node(\"model\",call_model)\n",
    "    \n",
    "    # Add Memmory\n",
    "    \n",
    "    memory = MemorySaver()\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "    return app\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
